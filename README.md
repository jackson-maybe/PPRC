# ğŸ›°ï¸ Efficient and Secure Range Counting over Distributed Geographic Data with Query Range Protection
This repository contains the implementation of **PPRC**, a Privacy-Preserving Range Counting protocol designed for secure range queries over distributed datasets. Range counting is a fundamental operation in geographic information systems (GIS) and various analytics tasks, yet it raises significant privacy concerns when data is held by multiple independent data holders (DHs). Existing privacy-preserving solutions either assume disjoint datasets, causing large estimation errors when overlaps occur, or expose the plaintext query range to DHs, thus compromising the privacy of query users. PPRC is the first protocol that addresses both issues while achieving practical efficiency. It reformulates secure point-in-range evaluation as encrypted membership tests implemented via encrypted Bloom filters, enabling efficient range queries without revealing the query range. Moreover, it employs an optimized secure count estimation sketch to aggregate overlapped results with minimal cryptographic cost, ensuring that no information beyond the final range count is leaked. Theoretical analysis and extensive experiments on real-world and synthetic datasets demonstrate that PPRC achieves up to 15Ã— smaller errors and 37Ã— faster performance than existing baselines.

## ğŸš€ Key Features
- **Query Range Protection** â€“ The query range is encrypted and never revealed to data holders.  
- **Overlap-Aware Counting** â€“ Correctly aggregates results across overlapping datasets without double-counting.  
- **Optimized Secure Sketching** â€“ Employs a lightweight encrypted sketch to minimize cryptographic cost.  
- **Accurate** â€“ Achieves up to **15Ã— smaller estimation errors** than  baselines.  
- **Efficient** â€“ Achieves up to **37Ã— faster runtime** (end-to-end) compared to baselines.


## ğŸ“‚ Repository Structure
```text
PPRC/
â”œâ”€â”€ datasets/       # Datasets used in the evaluation
â”‚   â”œâ”€â”€ brightkite/         # The Brightkite dataset
â”‚   â”œâ”€â”€ gowalla/            # The Gowalla dataset
â”‚   â”œâ”€â”€ spatial/            # The Yelp dataset
â”‚   â””â”€â”€ synthetic_datasets/ # The synthetic datasets 
â”œâ”€â”€ experiment_acc/ # Accuracy evaluation scripts (Python)
â”‚   â”œâ”€â”€ bloom_filter.py     # Python implementation of Bloom Filter
â”‚   â”œâ”€â”€ data_provider.py    # Simulates data holders and their local datasets
â”‚   â”œâ”€â”€ linear_counting.py  # Python implementation of Linear Counting
â”‚   â””â”€â”€ pprc.py             # Main script to run the PPRC
â”œâ”€â”€ MurmurHash3.cpp # Hash function implementation
â”œâ”€â”€ MurmurHash3.h   # Hash function header
â”œâ”€â”€ README.md
â”œâ”€â”€ SHE.cpp         # SHE scheme implementation
â”œâ”€â”€ SHE.h           # SHE header
â”œâ”€â”€ bloomfilter.cpp # Bloom filter implementation
â”œâ”€â”€ bloomfilter.h   # Bloom filter header
â”œâ”€â”€ linearcounting.cpp # Linear counting sketch implementation
â”œâ”€â”€ linearcounting.h # Linear counting header
â”œâ”€â”€ client.cpp # Query user (QU) client
â”œâ”€â”€ center.cpp # central aggregator (CA)
â”œâ”€â”€ server.cpp # Data holder (DH) server
â””â”€â”€ requirements.txt # Python dependencies
```

## ğŸ“Š Datasets
We evaluate PPRC on three real-world and one synthetic dataset to assess both accuracy and efficiency.
The real-world datasetsâ€”Yelp, Brightkite, and Gowallaâ€”cover diverse geographic and social contexts.
The synthetic dataset is uniformly generated within the San Francisco region (lat: 37.5â€“37.9, lon: â€“122.6â€“â€“122.2) at four scales: 10K, 100K, 1M, and 10M records.

| Dataset     | Type            | Records | Description |
|--------------|-----------------|---------:|-------------|
| Yelp         | Business        |   21,900 | Geographic business records (Florida) |
| Brightkite   | Social Network  |  115,383 | User check-in records |
| Gowalla      | Social Network  |  196,561 | User check-in records |
| Synthetic    | Generated       |  10Kâ€“10M | Uniformly sampled within the San Francisco region (lat: 37.5â€“37.9, lon: â€“122.6â€“â€“122.2) |





## âš™ï¸ Experimental Metrics
Our experiments measure both accuracy and efficiency. Accuracy is evaluated using **Mean Absolute Error (MAE)** and **Mean Relative Error (MRE)** between the estimated and true range counts. Efficiency is assessed by measuring the **end-to-end running time** (from query generation to result retrieval) and the **communication overhead** among the query user (QU), the central aggregator (CA), and the data holders (DHs).
## ğŸ§ª Running Accuracy Experiments
**1. Install Python dependencies**
 ```bash
   pip install -r requirements.txt
```
**2. Run accuracy evaluation**

``` bash
cd ./experiment_acc
python pprc.py
```
## âš¡ Running Efficiency Experiments
**1. Install dependencies**

``` bash
sudo apt update
sudo apt install build-essential libboost-all-dev libgmp-dev
```
   
**2. Compile the components**

``` bash
# Query user 
g++ -std=c++17 -o client client.cpp bloomfilter.cpp SHE.cpp MurmurHash3.cpp -lboost_system -lgmpxx -lgmp

# Data holders
g++ -std=c++17  -o server server.cpp MurmurHash3.cpp -lboost_system -lgmpxx -lgmp

# Central aggregator 
g++ -std=c++17 -o center center.cpp -lboost_system -lgmpxx -lgmp
```
   
**3. Run PPRC in three terminals**

Terminal 1 â€“ Start the Central Aggregator (CA)

``` bash
./center <listen_port_CA> <server_ip> <server_port>
# Example:
./center 9001 127.0.0.1 9002
```
Terminal 2 â€“ Start the Data Holders (DHs)

``` bash
./server <listen_port_DH>
# Example:
./server 9002
```


Terminal 3 â€“ Start the Query User (QU)
``` bash
./client <CA_ip> <CA_port>
# Example:
./client 127.0.0.1 9001
```
